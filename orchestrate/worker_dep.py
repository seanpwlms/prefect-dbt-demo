from pathlib import Path
from prefect.task_runners import SequentialTaskRunner
from prefect.task_runners import ConcurrentTaskRunner
from prefect_dbt_flow import dbt_flow
from prefect_dbt_flow.dbt import DbtProfile, DbtProject
from prefect import task, flow, runtime
from prefect_snowflake.database import SnowflakeConnector
from datetime import timedelta
from prefect.deployments import run_deployment
from prefect_dbt_flow.dbt import DbtProfile, DbtProject, DbtDagOptions
import time
import datetime

# Prefect block for configuration storage and retrieval ğŸ“¦
snowflake_connector = SnowflakeConnector.load("snowflake-demo-connector")


# Airbyte Connection Task ğŸ™
@task(name="ğŸ™ Airbyte Connection Task")
def airbyte_connection_task():
    print("Running Airbyte Task ğŸ™")
    time.sleep(3)
    return {"airbyte_data": "airbyte_data"}


# Snowflake Task ğŸ”
@task(name="â„ï¸ Snowflake Data Freshness Check", cache_expiration=timedelta(minutes=30))
def count_recent_cc_records():
    print("Running Snowflake Task â„ï¸")
    result = snowflake_connector.fetch_one(
        "select count(1) from snowflake_sample_data.tpcds_sf10tcl.call_center where cc_rec_start_date > current_date - 1"
    )
    time.sleep(1)

    return result


# Dynamically build DBT flow ğŸ› 
my_dbt_flow = dbt_flow(
    project=DbtProject(
        name="sample_project",
        project_dir=Path() / "prefect_demo",
        profiles_dir=Path.home() / ".dbt",
    ),
    profile=DbtProfile(
        target="prod",
    ),
    # Causes DBT Subflow to Fail
    dag_options=DbtDagOptions(
        run_test_after_model=True,
    ),
    flow_kwargs={
        "name": "ğŸ“ˆ DBT Subflow",
        "task_runner": ConcurrentTaskRunner(),
    },
)


@task(name="ğŸ“Š Update Dashboard")
def update_dashboard(transformed_data):
    print("Updating dashboard with transformed data! ğŸ“Š")
    time.sleep(3)


@task(name="ğŸ’Œ Send Confirmation Emails")
def confirmation_emails():
    print("Sending confirmation emails! ğŸ’Œ")
    time.sleep(3)


@flow(name="ğŸ“¦ Shipping Flow")
def shipping_flow():
    print("Kicking off the Shipping Flow ğŸ“¦")
    time.sleep(3)
    return {"shipping_data": "shipping_data"}


# ----------------------------------------------


# parent orchestrator flow ğŸ»
@flow(name="ğŸ» dbt Orchestrator Flow with Retry", log_prints=True, persist_result=True)
def dbt_orchestrator_retries_enabled(simulate_failure: bool = False):
    if runtime.flow_run.run_count > 1:
        simulate_failure = False

    # airbyte connection task ğŸ™
    data_transfer = airbyte_connection_task.submit()

    # snowflake task ğŸ”
    fresh_data = count_recent_cc_records.submit(wait_for=[data_transfer])

    if fresh_data is not None:
        # dbt subflow ğŸŸ¢
        if simulate_failure:
            my_dbt_flow = dbt_flow(
                project=DbtProject(
                    name="sample_project",
                    project_dir=Path() / "prefect_demo",
                    profiles_dir=Path.home() / ".dbt",
                ),
                profile=DbtProfile(
                    target="prod",
                ),
                # Causes DBT Subflow to Fail
                dag_options=DbtDagOptions(
                    run_test_after_model=True,
                ),
                flow_kwargs={
                    "name": "ğŸ“ˆ DBT Subflow",
                    "task_runner": ConcurrentTaskRunner(),
                },
            )
            transformed_data = my_dbt_flow(wait_for=[fresh_data])
        else:
            my_dbt_flow = dbt_flow(
                project=DbtProject(
                    name="sample_project",
                    project_dir=Path() / "prefect_demo",
                    profiles_dir=Path.home() / ".dbt",
                ),
                profile=DbtProfile(
                    target="prod",
                ),
                flow_kwargs={
                    "name": "ğŸ“ˆ dbt subflow",
                    "task_runner": ConcurrentTaskRunner(),
                },
            )
            transformed_data = my_dbt_flow(wait_for=[fresh_data])

    else:
        print("No fresh data found, scheduling another run in 15 minutes.")
        # Schedule another flow run 15 minutes from now. ğŸ”
        fifteen_minutes_from_now = datetime.now() + timedelta(minutes=15)
        run_deployment("dbt-parent-flow", scheduled_time=fifteen_minutes_from_now)

    # update dashboard ğŸ“Š
    update_dashboard.submit(transformed_data)

    # kick off shipping subflow ğŸ“¦
    shipping_data = shipping_flow(wait_for=[transformed_data])

    # send confirmation emails ğŸ’Œ
    confirmation_emails.submit(wait_for=shipping_data)


if __name__ == "__main__":
    dbt_orchestrator_retries_enabled(
        simulate_failure=True
    )  # Run once for development and testing

    # dbt_orchestrator_retries_enabled.serve("retry-deployment", interval=1800, tags=['worker']) # Interval Schedule of 30 minutes
